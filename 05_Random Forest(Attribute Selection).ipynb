{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Seung-hwanSong/Tree.git #코랩 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [의사결정나무 및 앙상블 Part 1]\n",
    "## Random Forest - Feature Importance (Attribute Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수백 또는 수백만 개의 feature 중, 가장 중요한 feature만 포함하는 모델을 만드는 것\n",
    "\n",
    "- 모델을 더 쉽게 해석 할 수 있음\n",
    "- 모델의 분산을 줄일 수 있음\n",
    "- 모델 학습 과정에서의 computational cost를 줄일 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 데이터 전처리 \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" 모델 생성, 학습, 평가 \"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 (kaggle data)\n",
    "data = pd.read_csv('/content/Tree/data/otto_train.csv')\n",
    "# data = pd.read_csv(\"./data/otto_train.csv\") #로컬\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무의미한 변수 제거\n",
    "\n",
    "data= data.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 변수의 형변환\n",
    "\n",
    "mapping_dict = {'Class_1' : 1,\n",
    "                'Class_2' : 2,\n",
    "                'Class_3' : 3,\n",
    "                'Class_4' : 4,\n",
    "                'Class_5' : 5,\n",
    "                'Class_6' : 6,\n",
    "                'Class_7' : 7,\n",
    "                'Class_8' : 8,\n",
    "                'Class_9' : 9,}\n",
    "after_mapping_target = data['target'].apply(lambda x : mapping_dict[x])\n",
    "after_mapping_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features/target, train/test dataset 분리\n",
    "\n",
    "feature_columns = list(data.columns.difference(['target']))\n",
    "X = data[feature_columns]\n",
    "y = after_mapping_target\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 2024) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Random Forest 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 300, max_depth = 100, criterion = 'gini', random_state = 1, n_jobs = -1)\n",
    "\n",
    "forest.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도 저장 \n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Featrue ranking: \")\n",
    "\n",
    "for f in range(train_x.shape[1]):\n",
    "    print(\"{}. feature {} ({:.3f})\".format(f + 1, train_x.columns[indices][f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib 을 이용한 plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_x.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(train_x.shape[1]), train_x.columns[indices], rotation=45)\n",
    "plt.xlim([-1, train_x.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 계산된 feature importance를 얼마만큼 신뢰할 수 있을까?\n",
    "\n",
    "- train_x에 random 열을 만들어, 무작위로 값을 넣음\n",
    "- Fitting이 되더라도 random 열의 feature importance는 낮아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['random'] = np.random.random(size=len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 동일한 모델로 훈련 \n",
    "forest_rand = RandomForestClassifier(n_estimators = 300, max_depth = 100, criterion = 'gini', random_state = 2024, n_jobs = -1)\n",
    "\n",
    "forest_rand.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest_rand.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest_rand.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Featrue ranking: \")\n",
    "\n",
    "for f in range(train_x.shape[1]):\n",
    "    print(\"{}. feature {} ({:.3f})\".format(f + 1, train_x.columns[indices][f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전혀 상관이 없어야 할 random 열의 feature importance가 다소 높은것을 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_x.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(train_x.shape[1]), train_x.columns[indices], rotation=45)\n",
    "plt.xlim([-1, train_x.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Problem\n",
    "- Gini Impurity 기반의 feature importance는 기본적으로 연속형 변수나, 많은 범주를 가진(high-cardinality) 변수의 importance를 과대평가하는 경향이 존재\n",
    "- 이러한 변수들이 노드 부닉의 기준이 될 기회가 많아서 발생하는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Permutation importance\n",
    "- 이러한 문제 상황에서 사용하는 방법으로, 해당 변수의 데이터를 임의로 섞는 permutation importance가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importances(rf, train_x, train_y, metric):\n",
    "    baseline = metric(rf, train_x, train_y)\n",
    "    imp = []\n",
    "    for col in train_x.columns:\n",
    "        save = train_x[col].copy()\n",
    "        train_x[col] = np.random.permutation(train_x[col])\n",
    "        m = metric(rf, train_x, train_y)\n",
    "        train_x[col] = save\n",
    "        imp.append(baseline - m)\n",
    "    return np.array(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from packaging.version import Version\n",
    "if Version(sklearn.__version__) >= Version(\"0.24\"):\n",
    "    # In sklearn version 0.24, forest module changed to be private.\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import _forest as forest\n",
    "else:\n",
    "    # Before sklearn version 0.24, forest was public, supporting this.\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import forest\n",
    "    \n",
    "def _get_unsampled_indices(tree, n_samples):\n",
    "    \"\"\"\n",
    "    An interface to get unsampled indices regardless of sklearn version.\n",
    "    \"\"\"\n",
    "    if Version(sklearn.__version__) >= Version(\"0.24\"):\n",
    "        # Version 0.24 moved forest package name\n",
    "        from sklearn.ensemble._forest import _get_n_samples_bootstrap\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "    elif Version(sklearn.__version__) >= Version(\"0.22\"):\n",
    "        # Version 0.22 or newer uses 3 arguments.\n",
    "        from sklearn.ensemble.forest import _get_n_samples_bootstrap\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "    else:\n",
    "        # Version 0.21 or older uses only two arguments.\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_classifier_accuracy(rf, train_x, train_y):\n",
    "    X = train_x.values\n",
    "    y = train_y.values\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in rf.estimators_:\n",
    "        unsampled_indices = _get_unsampled_indices(tree, n_samples)\n",
    "        tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "    predicted_class_indexes = np.argmax(predictions, axis=1)\n",
    "    predicted_classes = [rf.classes_[i] for i in predicted_class_indexes]\n",
    "\n",
    "    oob_score = np.mean(y == predicted_classes)\n",
    "    return oob_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일한 모델 적용 forest_rand\n",
    "\n",
    "imp = permutation_importances(forest_rand, train_x, train_y, oob_classifier_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_x.shape[1]), imp[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(train_x.shape[1]), train_x.columns[indices], rotation=45)\n",
    "plt.xlim([-1, train_x.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttest3.8",
   "language": "python",
   "name": "ttest3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
